
# ğŸ‡¹ğŸ‡· Llama-3-8B Turkish Instruct Model (KapalÄ± Devre LLM)

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![Library](https://img.shields.io/badge/Library-Unsloth%20%26%20HuggingFace-yellow)
![Hardware](https://img.shields.io/badge/Hardware-T4%20GPU%20Compatible-green)
![License](https://img.shields.io/badge/License-MIT-red)

> **Ostim Teknik Ãœniversitesi - Yapay Zeka MÃ¼hendisliÄŸi**
> **Ders: BÃ¼yÃ¼k Dil Modelleri (LLM)**
> **Ã–ÄŸrenci: Serhat TileklioÄŸlu (220212010)**

## ğŸ“– Proje Ã–zeti
Bu proje, dÄ±ÅŸ kaynaklÄ± API (OpenAI, Anthropic vb.) baÄŸÄ±mlÄ±lÄ±ÄŸÄ± olmadan, yerel donanÄ±m Ã¼zerinde Ã§alÄ±ÅŸabilen TÃ¼rkÃ§e talimat takip (Instruction Following) yeteneÄŸine sahip bir BÃ¼yÃ¼k Dil Modeli geliÅŸtirmek amacÄ±yla yapÄ±lmÄ±ÅŸtÄ±r.

Meta'nÄ±n **Llama-3-8B** modeli temel alÄ±nmÄ±ÅŸ ve **Unsloth** kÃ¼tÃ¼phanesi kullanÄ±larak **QLoRA** tekniÄŸi ile optimize edilmiÅŸtir. Veri seti eriÅŸim kÄ±sÄ±tlarÄ± nedeniyle, proje kapsamÄ±nda Ã¶zgÃ¼n bir **Sentetik Veri Ãœretim HattÄ± (Synthetic Data Pipeline)** geliÅŸtirilmiÅŸtir.

## âš™ï¸ Teknik Mimari ve YÃ¶ntem

Proje Ã¼Ã§ ana aÅŸamadan oluÅŸmaktadÄ±r:
1.  **Veri Ãœretimi:** Python tabanlÄ± ÅŸablon motoru ile sentetik veri Ã¼retimi.
2.  **Fine-Tuning:** Unsloth ve LoRA ile modelin eÄŸitilmesi.
3.  **Deployment:** Modelin GGUF formatÄ±na Ã§evrilerek offline kullanÄ±ma hazÄ±r hale getirilmesi.

### 1. Sentetik Veri Ãœretimi (Engineering Solution)
AÃ§Ä±k kaynak TÃ¼rkÃ§e veri setlerindeki eriÅŸim sorunlarÄ± nedeniyle (HTTP 404/401), kural tabanlÄ± bir veri Ã¼retim mekanizmasÄ± tasarlanmÄ±ÅŸtÄ±r. Bu mekanizma ile aÅŸaÄŸÄ±daki kategorilerde **2.000+** adet yÃ¼ksek kaliteli eÄŸitim verisi saniyeler iÃ§inde oluÅŸturulmuÅŸtur:
* **Genel KÃ¼ltÃ¼r:** BaÅŸkent-Ãœlke eÅŸleÅŸmeleri.
* **Matematik:** Rastgele sayÄ± Ã¼retimi ile toplama/Ã§arpma iÅŸlemleri.
* **Teknik SÃ¶zlÃ¼k:** YazÄ±lÄ±m ve Yapay Zeka terimlerinin tanÄ±mlarÄ±.

### 2. Model Optimizasyonu (QLoRA)
16GB VRAM (Tesla T4) kÄ±sÄ±tÄ± altÄ±nda 8 milyar parametreli bir modeli eÄŸitmek iÃ§in **Quantized Low-Rank Adaptation (QLoRA)** kullanÄ±lmÄ±ÅŸtÄ±r.
* **4-bit Quantization:** Model aÄŸÄ±rlÄ±klarÄ± sÄ±kÄ±ÅŸtÄ±rÄ±larak bellek kullanÄ±mÄ± dÃ¼ÅŸÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.
* **LoRA Rank (r):** 16
* **LoRA Alpha:** 16
* **Target Modules:** q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj

## ğŸš€ Kurulum ve KullanÄ±m

Bu projeyi yerel makinenizde veya Google Colab Ã¼zerinde Ã§alÄ±ÅŸtÄ±rmak iÃ§in aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin.

### Gereksinimler
```bash
pip install "unsloth[colab-new] @ git+[https://github.com/unslothai/unsloth.git](https://github.com/unslothai/unsloth.git)"
pip install --no-deps "xformers<0.0.26" trl peft accelerate bitsandbytes

```

### Modeli Ã‡alÄ±ÅŸtÄ±rma (Python)

EÄŸitilmiÅŸ modeli kullanmak iÃ§in Ã¶rnek kod bloÄŸu:

```python
from unsloth import FastLanguageModel

# Modeli ve Tokenizer'Ä± YÃ¼kle
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "model_cikti", # Veya GGUF dosya yolu
    max_seq_length = 2048,
    dtype = None,
    load_in_4bit = True,
)

# Inference (Soru Sorma)
FastLanguageModel.for_inference(model)
inputs = tokenizer(
    ["""### Talimat:\nDocker nedir?\n\n### YanÄ±t:\n"""], 
    return_tensors = "pt"
).to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 128)
print(tokenizer.batch_decode(outputs)[0])

```

## ğŸ“Š EÄŸitim SonuÃ§larÄ±

Model, eÄŸitim sÃ¼recinde ÅŸablonlarÄ± baÅŸarÄ±yla Ã¶ÄŸrenmiÅŸ ve Loss deÄŸerini dramatik ÅŸekilde dÃ¼ÅŸÃ¼rmÃ¼ÅŸtÃ¼r.

| AdÄ±m (Step) | Training Loss | Durum |
| --- | --- | --- |
| 1 | 3.1500 | BaÅŸlangÄ±Ã§ (Rastgele Cevaplar) |
| 30 | 0.1337 | Ã–ÄŸrenme AÅŸamasÄ± |
| 60 | **0.1271** | Final (YÃ¼ksek DoÄŸruluk) |

**Ã–rnek Ã‡Ä±ktÄ±lar:**

> **Soru:** Fransa'nÄ±n baÅŸkenti neresidir?
> **Model:** Fransa'nÄ±n baÅŸkenti Paris'tir.

> **Soru:** 25 ile 25 sayÄ±larÄ±nÄ±n toplamÄ± kaÃ§tÄ±r?
> **Model:** 25 + 25 = 50 eder.

## ğŸ“¥ Model Ä°ndirme (Download)

Github dosya boyutu sÄ±nÄ±rlarÄ± (Max 100MB) nedeniyle, eÄŸitilmiÅŸ ve GGUF formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ model dosyasÄ± harici sunucuda barÄ±ndÄ±rÄ±lmaktadÄ±r.

* **Format:** GGUF (q4_k_m)
* **Boyut:** ~4.9 GB
* **Uyumluluk:** llama.cpp, LM Studio, Ollama

[ğŸ‘‰ **MODELÄ° Ä°NDÄ°RMEK Ä°Ã‡Ä°N TIKLAYIN (Google Drive)](https://colab.research.google.com/drive/1hDwyGjiReqMmWrIxJSSy9pdaQFi_H8gZ?usp=sharing)**

## ğŸ“œ Lisans

Bu proje MIT lisansÄ± ile lisanslanmÄ±ÅŸtÄ±r. KullanÄ±lan temel model (Llama-3) Meta'nÄ±n lisans koÅŸullarÄ±na tabidir.

---

*Bu proje Ostim Teknik Ãœniversitesi Yapay Zeka MÃ¼hendisliÄŸi bÃ¶lÃ¼mÃ¼ bitirme/ders projesi kapsamÄ±nda hazÄ±rlanmÄ±ÅŸtÄ±r.*

```

